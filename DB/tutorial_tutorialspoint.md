# DBMS Tutorial
https://www.tutorialspoint.com/dbms/index.htm

## DBMS Overview

Characteristics
Traditionally, data was organized in file formats.

A modern DBMS has the following characteristics:
- Real-world entity
    A modern DBMS is more realistic and uses real-world entities to design its architecture. It uses the behavior and attributes too. For example, a school database may use students as an entity and their age as an attribute.

- Relation-based tables
    DBMS allows entities and relations among them to form tables. A user can understand the architecture of a database just by looking at the table names.

- Isolation of data and application
    A database system is entirely different than its data. A database is an active entity, whereas data is said to be passive, on which the database works and organizes. DBMS also stores metadata, which is data about data, to ease its own process.

- Less redundancy
    DBMS follows the rules of normalization, which splits a relation when any of its attributes is having redundancy in values. Normalization is a mathematically rich and scientific process that reduces data redundancy.

- Consistency

- Query Language
    DBMS is equipped with query language, which makes it more efficient to retrieve and manipulate data. A user can apply as many and as different filtering options as required to retrieve a set of data. Traditionally it was not possible where file-processing system was used.

- ACID Properties
    DBMS follows the concepts of Atomicity, Consistency, Isolation, and Durability (normally shortened as ACID). These concepts are applied on transactions, which manipulate data in a database. ACID properties help the database stay healthy in multi-transactional environments and in case of failure.

- Multiuser and Concurrent Access
    DBMS supports multi-user environment and allows them to access and manipulate data in parallel. Though there are restrictions on transactions when users attempt to handle the same data item, but users are always unaware of them.

- Multiple views
    DBMS offers multiple views for different users. A user who is in the Sales department will have a different view of database than a person working in the Production department. This feature enables the users to have a concentrate view of the database according to their requirements.

- Security



Users
- Administrators
     Administrators maintain the DBMS and are responsible for administrating the database. They are responsible to look after its usage and by whom it should be used. They create access profiles for users and apply limitations to maintain isolation and force security. Administrators also look after DBMS resources like system license, required tools, and other software and hardware related maintenance.

- Designers
    Designers are the group of people who actually work on the designing part of the database. They keep a close watch on what data should be kept and in what format. They identify and design the whole set of entities, relations, constraints, and views.

- End Users
    End users are those who actually reap the benefits of having a DBMS. End users can range from simple viewers who pay attention to the logs or market rates to sophisticated users such as business analysts.


## DBMS Architecture

3-tier Architecture
A 3-tier architecture separates its tiers from each other based on the complexity of the users and how they use the data present in the database. It is the most widely used architecture to design a DBMS.

Presentation Tier
||
Application Tier
||
Database Tier


- Database (Data) Tier
    At this tier, the database resides along with its query processing languages. We also have the relations that define the data and their constraints at this level.

- Application (Middle) Tier
    the application layer sits in the middle and acts as a mediator between the end-user and the database.

- User (Presentation) Tier
    End-users operate on this tier and they know nothing about any existence of the database beyond this layer. At this layer, multiple views of the database can be provided by the application. All views are generated by applications that reside in the application tier.

Multiple-tier database architecture is highly modifiable, as almost all its components are independent and can be changed independently.


## DBMS - Data Models
Data models define how the logical structure of a database is modeled. Data Models are fundamental entities to introduce abstraction in a DBMS. Data models define how data is connected to each other and how they are processed and stored inside the system.

The very first data model could be flat data-models, where all the data used are to be kept in the same plane. Earlier data models were not so scientific, hence they were prone to introduce lots of duplication and update anomalies.



Entity-Relationship Model
Entity-Relationship (ER) Model is based on the notion of real-world entities and relationships among them.

While formulating real-world scenario into the database model, the ER Model creates entity set, relationship set, general attributes and constraints.

ER Model is best used for the conceptual design of a database.

ER Model is based on:
- Entities and their attributes.
- Relationships among entities.

Concepts:
- Entity
    An entity in an ER Model is a real-world entity having properties called **attributes**. Every attribute is defined by its set of values called domain. For example, in a school database, a student is considered as an entity. Student has various attributes like name, age, class, etc.

- Relationship
    The logical association among entities is called relationship. Relationships are mapped with entities in various ways.
    Mapping cardinalities define the number of association between two entities.
    Mapping cardinalities:
    - one to one
    - one to many
    - many to one
    - many to many


Relational Model
The most popular data model in DBMS is the Relational Model. It is more scientific a model than others. This model is based on first-order predicate logic and defines a table as an **n-ary relation**.


## DBMS Data Schemas
A database schema is the skeleton structure that represents the logical view of the entire database.
It defines how the data is organized and how the relations among them are associated.
It formulates all the constraints that are to be applied on the data.

A database schema defines its entities and the relationship among them. It contains a descriptive detail of the database, which can be depicted by means of schema diagrams. It’s the database designers who design the schema to help programmers understand the database and make it useful.

A database schema can be divided broadly into two categories −
- Physical Database Schema
    This schema pertains to the actual storage of data and its form of storage like files, indices, etc. It defines how the data will be stored in a secondary storage.
- Logical Database Schema
    This schema defines all the logical constraints that need to be applied on the data stored. It defines tables, views, and integrity constraints.


DB schema vs DB instance
 Database schema is the skeleton of database. It is designed when the database doesn't exist at all. Once the database is operational, it is very difficult to make any changes to it. A database schema does not contain any data or information.

 Database instance is a state of operational database with data at any given time. It contains a snapshot of the database. Database instances tend to change with time. A DBMS ensures that its every instance (state) is in a valid state, by diligently following all the validations, constraints, and conditions that the database designers have imposed.


## DBMS Data Independence
If a database system is not multi-layered, then it becomes difficult to make any changes in the database system. Database systems are designed in multi-layers as we learnt earlier.


Data Independence
A database system normally contains a lot of data in addition to users’ data. For example, it stores data about data, known as metadata, to locate and retrieve data easily. It is rather difficult to modify or update a set of metadata once it is stored in the database.

But as a DBMS expands, it needs to change over time to satisfy the requirements of the users. If the entire data is dependent, it would become a tedious and highly complex job.

Logical Schema(Logical Data Independance)
Physical Schema(Physical Data Independance)

Metadata itself follows a layered architecture, so that when we change data at one layer, it does not affect the data at another level. This data is independent but mapped to each other.


Logical Data Independence

Physical Data Independence
All the schemas are logical, and the actual data is stored in bit format on the disk. Physical data independence is the power to change the physical data without impacting the schema or logical data.
For example, in case we want to change or upgrade the storage system itself − suppose we want to replace hard-disks with SSD − it should not have any impact on the logical data or schemas.


# Entity Relationship Model

## ER Model - Basic Concepts
The ER model defines the conceptual view of a database. It works around real-world entities and the associations among them. At view level, the ER model is considered a good option for designing databases.


Entity
An entity can be a real-world object, either animate or inanimate, that can be easily identifiable. For example, in a school database, students, teachers, classes, and courses offered can be considered as entities. All these entities have some attributes or properties that give them their identity.

An entity set is a collection of similar types of entities. An entity set may contain entities with attribute sharing similar values. For example, a Students set may contain all the students of a school; likewise a Teachers set may contain all the teachers of a school from all faculties. Entity sets need not be disjoint.


Attributes
Entities are represented by means of their properties, called attributes. All attributes have values. For example, a student entity may have name, class, and age as attributes.

There exists a domain or range of values that can be assigned to attributes. For example, a student's name cannot be a numeric value. It has to be alphabetic. A student's age cannot be negative, etc.

Types of Attributes
- Simple attribute
    Simple attributes are atomic values, which cannot be divided further. For example, a student's phone number is an atomic value of 10 digits.

- Composite attribute
    Composite attributes are made of more than one simple attribute. For example, a student's complete name may have first_name and last_name.

- Derived attribute
    Derived attributes are the attributes that do not exist in the physical database, but their values are derived from other attributes present in the database. For example, average_salary in a department should not be saved directly in the database, instead it can be derived. For another example, age can be derived from data_of_birth.

- Single-value attribute
    Single-value attributes contain single value. For example − Social_Security_Number.

- Multi-value attribute
    Multi-value attributes may contain more than one values. For example, a person can have more than one phone number, email_address, etc.


These attribute types can come together in a way like −
    simple single-valued attributes
    simple multi-valued attributes
    composite single-valued attributes
    composite multi-valued attributes


Entity-Set and Keys
Key is an attribute or collection of attributes that uniquely identifies an entity among entity set.
- Super Key
    A set of attributes (one or more) that collectively identifies an entity in an entity set.

- Candidate Key
    A minimal super key is called a candidate key. An entity set may have more than one candidate key.

- Primary Key
    A primary key is one of the candidate keys chosen by the database designer to uniquely identify the entity set.


Relationship
The association among entities is called a relationship.
For example, an employee works_at a department, a student enrolls in a course. Here, Works_at and Enrolls are called relationships.

Relationship Set
A set of relationships of similar type is called a relationship set. Like entities, a relationship too can have attributes. These attributes are called descriptive attributes.

Degree of Relationship
The number of participating entities in a relationship defines the degree of the relationship.
    Binary = degree 2
    Ternary = degree 3
    n-ary = degree n


Mapping Cardinalities
Cardinality defines the number of entities in one entity set, which can be associated with the number of entities of other set via relationship set.
    one to one
    one to many
    many to one
    many to many


## ER Diagram Representation
how the ER Model is represented by means of an ER diagram. Any object, for example, entities, attributes of an entity, relationship sets, and attributes of relationship sets, can be represented with the help of an ER diagram.

Entity
Entities are represented by means of rectangles. Rectangles are named with the entity set they represent.

Attributes
Attributes are the properties of entities. Attributes are represented by means of ellipses. Every ellipse represents one attribute and is directly connected to its entity (rectangle).

If the attributes are composite, they are further divided in a tree like structure. Every node is then connected to its attribute. That is, composite attributes are represented by ellipses that are connected with an ellipse.

Multivalued attributes are depicted by double ellipse.

Derived attributes are depicted by dashed ellipse.


Relationship
Relationships are represented by diamond-shaped box. Name of the relationship is written inside the diamond-box. All the entities (rectangles) participating in a relationship, are connected to it by a line.

Relation Mapping Cardinality
1-1
1-N
N-1
N-N


Participation Constraints
- Total Participation
    Each entity is involved in the relationship. Total participation is represented by double lines.

- Partial participation
    Not all entities are involved in the relationship. Partial participation is represented by single lines.



## Generalization Aggregation
The ER Model has the power of expressing database entities in a conceptual hierarchical manner. As the hierarchy goes up, it generalizes the view of entities, and as we go deep in the hierarchy, it gives us the detail of every entity included.

Going up in this structure is called **generalization**, where entities are clubbed together to represent a more generalized view. For example, a particular student named Mira can be generalized along with all the students. The entity shall be a student, and further, the student is a person.
The reverse is called **specialization** where a person is a student, and that student is Mira.

Generalization
As mentioned above, the process of generalizing entities, where the generalized entities contain the properties of all the generalized entities, is called generalization. In generalization, a number of entities are brought together into one generalized entity based on their similar characteristics.
For example, pigeon, house sparrow, crow and dove can all be generalized as Birds.


Specialization
Specialization is the opposite of generalization. In specialization, a group of entities is divided into sub-groups based on their characteristics.
Take a group ‘Person’ for example. A person has name, date of birth, gender, etc. These properties are common in all persons, human beings. But in a company, persons can be identified as employee, employer, customer, or vendor, based on w


Inheritance
We use all the above features of ER-Model in order to create classes of objects in object-oriented programming. The details of entities are generally hidden from the user; this process known as abstraction.

Inheritance is an important feature of Generalization and Specialization. It allows lower-level entities to inherit the attributes of higher-level entities.


# Relational Model

## DBMS - Codd's Rules
Dr Edgar F. Codd, after his extensive research on the Relational Model of database systems, came up with twelve rules of his own, which according to him, a database must obey in order to be regarded as a true relational database.

These rules can be applied on any database system that manages stored data using only its relational capabilities. This is a foundation rule, which acts as a base for all the other rules.

Rule 1: Information Rule
The data stored in a database, may it be user data or metadata, must be a value of some table cell. Everything in a database must be stored in a table format.


Rule 2: Guaranteed Access Rule
Every single data element (value) is guaranteed to be accessible logically with a combination of table-name, primary-key (row value), and attribute-name (column value). No other means, such as pointers, can be used to access data.


Rule 3: Systematic Treatment of NULL Values
The NULL values in a database must be given a systematic and uniform treatment. This is a very important rule because a NULL can be interpreted as one the following − data is missing, data is not known, or data is not applicable.

Rule 4: Active Online Catalog
The structure description of the entire database must be stored in an online catalog, known as data dictionary, which can be accessed by authorized users. Users can use the same query language to access the catalog which they use to access the database itself.

Rule 5: Comprehensive Data Sub-Language Rule
A database can only be accessed using a language having linear syntax that supports data definition, data manipulation, and transaction management operations. This language can be used directly or by means of some application. If the database allows access to data without any help of this language, then it is considered as a violation.

Rule 6: View Updating Rule
All the views of a database, which can theoretically be updated, must also be updatable by the system.

Rule 7: High-Level Insert, Update, and Delete Rule
A database must support high-level insertion, updation, and deletion. This must not be limited to a single row, that is, it must also support union, intersection and minus operations to yield sets of data records.

Rule 8: Physical Data Independence
The data stored in a database must be independent of the applications that access the database. Any change in the physical structure of a database must not have any impact on how the data is being accessed by external applications.

Rule 9: Logical Data Independence
The logical data in a database must be independent of its user’s view (application). Any change in logical data must not affect the applications using it. For example, if two tables are merged or one is split into two different tables, there should be no impact or change on the user application. This is one of the most difficult rule to apply.

Rule 10: Integrity Independence
A database must be independent of the application that uses it. All its integrity constraints can be independently modified without the need of any change in the application. This rule makes a database independent of the front-end application and its interface.

Rule 11: Distribution Independence
The end-user must not be able to see that the data is distributed over various locations. Users should always get the impression that the data is located at one site only. This rule has been regarded as the foundation of distributed database systems.

Rule 12: Non-Subversion Rule
If a system has an interface that provides access to low-level records, then the interface must not be able to subvert the system and bypass security and integrity constraints.


## Relation Data Model

Concepts
- Tables
- Tuple
- Relation instance
    A finite set of tuples in the relational database system represents relation instance. Relation instances do not have duplicate tuples.
- Relation schema
    A relation schema describes the relation name (table name), attributes, and their names.
- Relation key
    Each row has one or more attributes, known as relation key, which can identify the row in the relation (table) uniquely.
- Attribute domain
    Every attribute has some pre-defined value scope, known as attribute domain.


Constraints
Every relation has some conditions that must hold for it to be a valid relation. These conditions are called Relational Integrity Constraints.
There are 3 main integrity constraints −
    Key constraints
    Domain constraints
    Referential integrity constraints

Key Constraints
There must be at least one minimal subset of attributes in the relation, which can identify a tuple uniquely. This minimal subset of attributes is called key for that relation. If there are more than one such minimal subsets, these are called candidate keys.


Domain Constraints
Attributes have specific values in real-world scenario.
For example, age can only be a positive integer. The same constraints have been tried to employ on the attributes of a relation. Every attribute is bound to have a specific range of values. For example, age cannot be less than zero and telephone numbers cannot contain a digit outside 0-9.

Referential integrity Constraints
Referential integrity constraints work on the concept of Foreign Keys. A foreign key is a key attribute of a relation that can be referred in other relation.

Referential integrity constraint states that if a relation refers to a key attribute of a different or same relation, then that key element must exist.


## Relational Algebra

Relational database systems are expected to be equipped with a query language that can assist its users to query the database instances.
There are 2 kinds of query languages:
    1) relational algebra
    2) relational calculus

Relational Algebra
Relational algebra is a procedural query language, which takes instances of relations as input and yields instances of relations as output.

It uses operators to perform queries. An operator can be either unary or binary. They accept relations as their input and yield relations as their output. Relational algebra is performed recursively on a relation and intermediate results are also considered relations.

The fundamental operations of relational algebra are as follows −
    Select
    Project
    Union
    Set different
    Cartesian product
    Rename


Relational Calculus
In contrast to Relational Algebra, Relational Calculus is a non-procedural query language, that is, it tells what to do but never explains how to do it.


## ER Model to Relational Model

ER Model, when conceptualized into diagrams, gives a good overview of entity-relationship, which is easier to understand. ER diagrams can be mapped to relational schema, that is, it is possible to create relational schema using ER diagram. We cannot import all the ER constraints into relational model, but an approximate schema can be generated.

There are several processes and algorithms available to convert ER Diagrams into Relational Schema. Some of them are automated and some of them are manual. We may focus here on the mapping diagram contents to relational basics.

ER diagrams mainly comprise of −
    Entity and its attributes
    Relationship, which is association among entities.


Mapping Entity
An entity is a real-world object with some attributes.

Mapping Process (Algorithm)
    - Create table for each entity.
    - Entity's attributes should become fields of tables with their respective data types.
    - Declare primary key.


Mapping Relationship
A relationship is an association among entities.

Mapping Process
    - Create table for a relationship.
    - Add the primary keys of all participating Entities as fields of table with their respective data types.
    - If relationship has any attribute, add each attribute as field of table.
    - Declare a primary key composing all the primary keys of participating entities.
    - Declare all foreign key constraints.


Mapping Weak Entity Sets
A weak entity set is one which does not have any primary key associated with it.

Mapping Process
    - Create table for weak entity set.
    - Add all its attributes to table as field.
    - Add the primary key of identifying entity set.
    - Declare all foreign key constraints.


Mapping Hierarchical Entities
ER specialization or generalization comes in the form of hierarchical entity sets.

Mapping Process
    - Create tables for all higher-level entities.
    - Create tables for lower-level entities.
    - Add primary keys of higher-level entities in the table of lower-level entities.
    - In lower-level tables, add all other attributes of lower-level entities.
    - Declare primary key of higher-level table and the primary key for lower-level table.
    - Declare foreign key constraints.


## SQL Overview
SQL is a programming language for Relational Databases. It is designed over relational algebra and tuple relational calculus.

SQL comes as a package with all major distributions of RDBMS.

SQL comprises both
- data definition
- data manipulation languages.

Using the data definition properties of SQL, one can design and modify database schema,
whereas data manipulation properties allows SQL to store and retrieve data from database.

Data Definition Language (DDL)
- CREATE
- DROP
- ALTER


Data Manipulation Language (DML)
- SELECT/FROM/WHERE
    SELECT author_name FROM book_author WHERE age > 50;
- INSERT INTO/VALUES
    INSERT INTO tutorialspoint (Author, Subject) VALUES ("anonymous", "computers");
- UPDATE/SET/WHERE
    UPDATE tutorialspoint SET Author="webmaster" WHERE Author="anonymous";
- DELETE FROM/WHERE
    DELETE FROM tutorialspoints WHERE Author="unknown";


# Relational Database Design

## DBMS Normalization

Functional Dependency
Functional dependency (FD) is a set of constraints between two attributes in a relation.

Functional dependency says that if two tuples have same values for attributes A1, A2,..., An, then those two tuples must have to have same values for attributes B1, B2, ..., Bn.

Functional dependency is represented by an arrow sign (→) that is, X→Y, where X functionally determines Y. The left-hand side attributes determine the values of attributes on the right-hand side.


Armstrong's Axioms


Trivial Functional Dependency
Trivial − If a functional dependency (FD) X → Y holds, where Y is a subset of X, then it is called a trivial FD. Trivial FDs always hold.

Non-trivial − If an FD X → Y holds, where Y is not a subset of X, then it is called a non-trivial FD.

Completely non-trivial − If an FD X → Y holds, where x intersect Y = Φ, it is said to be a completely non-trivial FD.



Normalization

First Normal Form
This rule defines that all the attributes in a relation must have atomic domains. The values in an atomic domain are indivisible units.


Second Normal Form (no partial dependency.)
Prime attribute − An attribute, which is a part of the candidate-key, is known as a prime attribute.

Non-prime attribute − An attribute, which is not a part of the prime-key, is said to be a non-prime attribute.

every non-prime attribute should be fully functionally dependent on prime key attribute. That is, if X → A holds, then there should not be any proper subset Y of X, for which Y → A also holds true.


 Third Normal Form (no transitive dependency.)
No non-prime attribute is transitively dependent on prime key attribute.
For any non-trivial functional dependency, X → A, then either −
X is a superkey or,
A is prime attribute.


Boyce-Codd Normal Form
Boyce-Codd Normal Form (BCNF) is an extension of Third Normal Form on strict terms. BCNF states that −
For any non-trivial functional dependency, X → A, X must be a super-key.


## DBMS - Joins
Theta (θ) Join
Theta join combines tuples from different relations provided they satisfy the theta condition. The join condition is denoted by the symbol θ.

Equijoin
When Theta join uses only equality comparison operator, it is said to be equijoin. The above example corresponds to equijoin.

Natural Join (⋈)
Natural join does not use any comparison operator. It does not concatenate the way a Cartesian product does. We can perform a Natural Join only if there is at least one common attribute that exists between two relations. In addition, the attributes must have the same name and domain.

Natural join acts on those matching attributes where the values of attributes in both the relations are same.

Inner join
- Theta Join
- Equijoin
- Natural Join

Outer Joins
Theta Join, Equijoin, and Natural Join are called inner joins. An inner join includes only those tuples with matching attributes and the rest are discarded in the resulting relation. Therefore, we need to use outer joins to include all the tuples from the participating relations in the resulting relation.

Outer Join
- left outer join
- right outer join
- full outer join


## DBMS Storage System

Databases are stored in file formats, which contain records. At physical level, the actual data is stored in electromagnetic format on some device. These storage devices can be broadly categorized into three types −
1) Primary Storage: RAM
2) Secondary Storage: hard disk, flash drive, etc.
3) Tertiary Storage: magnetic tapes..


Memory Hierarchy
A computer system has a well-defined hierarchy of memory. A CPU has direct access to it main memory as well as its inbuilt registers. The access time of the main memory is obviously less than the CPU speed. To minimize this speed mismatch, cache memory is introduced. Cache memory provides the fastest access time and it contains data that is most frequently accessed by the CPU.


Magnetic Disks
Hard disk drives are the most common secondary storage devices in present computer systems. These are called magnetic disks because they use the concept of magnetization to store information. Hard disks consist of metal disks coated with magnetizable material. These disks are placed vertically on a spindle. A read/write head moves in between the disks and is used to magnetize or de-magnetize the spot under it. A magnetized spot can be recognized as 0 (zero) or 1 (one).

Hard disks are formatted in a well-defined order to store data efficiently. A hard disk plate has many concentric circles on it, called tracks. Every track is further divided into sectors. A sector on a hard disk typically stores 512 bytes of data.


RAID(Redundant Array of Independent Disks)
RAID or Redundant Array of Independent Disks, is a technology to connect multiple secondary storage devices and use them as a single storage media.



## DBMS - File Structure
Relative data and information is stored collectively in file formats.
A file is a sequence of records stored in binary format. A disk drive is formatted into several blocks that can store records. File records are mapped onto those disk blocks.


File Organization
File Organization defines how file records are mapped onto disk blocks.
We have four types of File Organization to organize file records −
    Heap File Organization
    Sequential File Organization
    Hash File Organization
    Clustered File Organization


File Operations
Operations on database files can be broadly classified into two categories −
    Update Operations
    Retrieval Operations

Update operations change the data values by insertion, deletion, or update.
Retrieval operations, on the other hand, do not alter the data but retrieve them after optional conditional filtering.

In both types of operations, selection plays a significant role. Other than creation and deletion of a file, there could be several operations, which can be done on files.

Open − A file can be opened in one of the two modes, read mode or write mode.

Locate − Every file has a file pointer, which tells the current position where the data is to be read or written. This pointer can be adjusted accordingly. Using find (seek) operation, it can be moved forward or backward.

Read − By default, when files are opened in read mode, the file pointer points to the beginning of the file. There are options where the user can tell the operating system where to locate the file pointer at the time of opening a file. The very next data to the file pointer is read.

Write − User can select to open a file in write mode, which enables them to edit its contents. It can be deletion, insertion, or modification. The file pointer can be located at the time of opening or can be dynamically changed if the operating system allows to do so.

Close − This is the most important operation from the operating system’s point of view. When a request to close a file is generated, the operating system
    removes all the locks (if in shared mode),
    saves the data (if altered) to the secondary storage media, and
    releases all the buffers and file handlers associated with the file.


# Indexing and Hashing

## DBMS Indexing
Indexing is a data structure technique to efficiently retrieve records from the database files based on some attributes on which the indexing has been done.

Indexing is defined based on its indexing attributes.
Indexing can be of the following types −
- Primary Index
    Primary index is defined on an ordered data file. The data file is ordered on a key field. The key field is generally the primary key of the relation.

- Secondary Index
    Secondary index may be generated from a field which is a candidate key and has a unique value in every record, or a non-key with duplicate values.

- Clustering Index
    Clustering index is defined on an ordered data file. The data file is ordered on a non-key field.

Ordered Indexing is of two types −
    Dense Index
    Sparse Index


Dense Index
In dense index, there is an index record for every search key value in the database. This makes searching faster but requires more space to store index records itself. Index records contain search key value and a pointer to the actual record on the disk.

Sparse Index
In sparse index, index records are not created for every search key. An index record here contains a search key and an actual pointer to the data on the disk. To search a record, we first proceed by index record and reach at the actual location of the data. If the data we are looking for is not where we directly reach by following the index, then the system starts sequential search until the desired data is found.

Multilevel Index
Index records comprise search-key values and data pointers. Multilevel index is stored on the disk along with the actual database files. As the size of the database grows, so does the size of the indices. There is an immense need to keep the index records in the main memory so as to speed up the search operations. If single-level index is used, then a large size index cannot be kept in memory which leads to multiple disk accesses.

Multi-level Index helps in breaking down the index into several smaller indices in order to make the outermost level so small that it can be saved in a single disk block, which can easily be accommodated anywhere in the main memory.

B+ Tree
A B+ tree is a balanced binary search tree that follows a multi-level index format. The leaf nodes of a B+ tree denote actual data pointers. B+ tree ensures that all leaf nodes remain at the same height, thus balanced. Additionally, the leaf nodes are linked using a link list; therefore, a B+ tree can support random access as well as sequential access.


## DBMS - Hashing

For a huge database structure, it can be almost next to impossible to search all the index values through all its level and then reach the destination data block to retrieve the desired data.

Hashing is an effective technique to calculate the direct location of a data record on the disk without using index structure.

Hashing uses hash functions with search keys as parameters to generate the address of a data record.


Hash Organization
- Bucket
    A hash file stores data in bucket format. Bucket is considered a unit of storage. A bucket typically stores one complete disk block, which in turn can store one or more records.

- Hash Function
    A hash function, h, is a mapping function that maps all the set of search-keys K to the address where actual records are placed. It is a function from search keys to bucket addresses.


Static Hashing
Operation
- Insertion
    When a record is required to be entered using static hash, the hash function h computes the bucket address for search key K, where the record will be stored.

    Bucket address = h(K)

- Search
    When a record needs to be retrieved, the same hash function can be used to retrieve the address of the bucket where the data is stored.

- Delete
    This is simply a search followed by a deletion operation.


Bucket Overflow
The condition of bucket-overflow is known as collision. This is a fatal state for any static hash function. In this case, overflow chaining can be used.

Overflow Chaining − When buckets are full, a new bucket is allocated for the same hash result and is linked after the previous one. This mechanism is called Closed Hashing.

Linear Probing − When a hash function generates an address at which data is already stored, the next free bucket is allocated to it. This mechanism is called Open Hashing.


Dynamic Hashing
The problem with static hashing is that it does not expand or shrink dynamically as the size of the database grows or shrinks. Dynamic hashing provides a mechanism in which data buckets are added and removed dynamically and on-demand. Dynamic hashing is also known as extended hashing.

Hash function, in dynamic hashing, is made to produce a large number of values and only a few are used initially.


Hashing is not favorable when the data is organized in some ordering and the queries require a range of data. When data is discrete and random, hash performs the best.

Hashing algorithms have high complexity than indexing. All hash operations are done in constant time.


# Transaction And Concurrency
A transaction can be defined as a group of tasks.
A single task is the minimum processing unit which cannot be divided further.

ACID Properties
- Atomicity
    This property states that a transaction must be treated as an atomic unit, that is, either all of its operations are executed or none. There must be no state in a database where a transaction is left partially completed. States should be defined either before the execution of the transaction or after the execution/abortion/failure of the transaction.

- Consistency
    The database must remain in a consistent state after any transaction. No transaction should have any adverse effect on the data residing in the database. If the database was in a consistent state before the execution of a transaction, it must remain consistent after the execution of the transaction as well.

- Isolation
    In a database system where more than one transaction are being executed simultaneously and in parallel, the property of isolation states that all the transactions will be carried out and executed as if it is the only transaction in the system. No transaction will affect the existence of any other transaction.

- Durability
    The database should be durable enough to hold all its latest updates even if the system fails or restarts. If a transaction updates a chunk of data in a database and commits, then the database will hold the modified data. If a transaction commits but the system fails before the data could be written on to the disk, then that data will be updated once the system springs back into action.


Serializability
When multiple transactions are being executed by the operating system in a multiprogramming environment, there are possibilities that instructions of one transactions are interleaved with some other transaction.

- Schedule
    A chronological execution sequence of a transaction is called a schedule. A schedule can have many transactions in it, each comprising of a number of instructions/tasks.

- Serial Schedule
    It is a schedule in which transactions are aligned in such a way that one transaction is executed first. When the first transaction completes its cycle, then the next transaction is executed. Transactions are ordered one after the other. This type of schedule is called a serial schedule, as transactions are executed in a serial manner.

we allow parallel execution of a transaction schedule, if its transactions are either serializable or have some equivalence relation among them.


Equivalence Schedules
An equivalence schedule can be of the following types −
1) Result Equivalence
    If two schedules produce the same result after execution, they are said to be result equivalent. They may yield the same result for some value and different results for another set of values. That's why this equivalence is not generally considered significant.

2) View Equivalence
    Two schedules would be view equivalence if the transactions in both the schedules perform similar actions in a similar manner.

3) Conflict Equivalence

Two schedules would be conflicting if they have the following properties −
    Both belong to separate transactions.
    Both accesses the same data item.
    At least one of them is "write" operation.

Two schedules having multiple transactions with conflicting operations are said to be conflict equivalent if and only if −

Both the schedules contain the same set of Transactions.
The order of conflicting pairs of operation is maintained in both the schedules.


States of Transactions
A transaction in a database can be in one of the following states −

- Active
    In this state, the transaction is being executed. This is the initial state of every transaction.

- Partially Committed
    When a transaction executes its final operation, it is said to be in a partially committed state.

- Failed
    A transaction is said to be in a failed state if any of the checks made by the database recovery system fails. A failed transaction can no longer proceed further.

- Aborted
    If any of the checks fails and the transaction has reached a failed state, then the recovery manager rolls back all its write operations on the database to bring the database back to its original state where it was prior to the execution of the transaction. Transactions in this state are called aborted. The database recovery module can select one of the two operations after a transaction aborts −
    - Re-start the transaction
    - Kill the transaction

- Committed
    If a transaction executes all its operations successfully, it is said to be committed. All its effects are now permanently established on the database system.



## DBMS - Concurrency Control

In a multiprogramming environment where multiple transactions can be executed simultaneously, it is highly important to control the concurrency of transactions. We have concurrency control protocols to ensure atomicity, isolation, and serializability of concurrent transactions. Concurrency control protocols can be broadly divided into two categories −
    Lock based protocols
    Time stamp based protocols


Lock-based Protocols
Locks are of two kinds −
- Binary Locks
    A lock on a data item can be in two states; it is either locked or unlocked.

- Shared/exclusive
    This type of locking mechanism differentiates the locks based on their uses. If a lock is acquired on a data item to perform a write operation, it is an exclusive lock. Allowing more than one transaction to write on the same data item would lead the database into an inconsistent state. Read locks are shared because no data value is being changed.


There are four types of lock protocols available −
1) Simplistic Lock Protocol
    Simplistic lock-based protocols allow transactions to obtain a lock on every object before a 'write' operation is performed. Transactions may unlock the data item after completing the ‘write’ operation.

2) Pre-claiming Lock Protocol
    Pre-claiming protocols evaluate their operations and create a list of data items on which they need locks. Before initiating an execution, the transaction requests the system for all the locks it needs beforehand. If all the locks are granted, the transaction executes and releases all the locks when all its operations are over. If all the locks are not granted, the transaction rolls back and waits until all the locks are granted.

3) Two-Phase Locking 2PL
    This locking protocol divides the execution phase of a transaction into three parts.
    In the first part, when the transaction starts executing, it seeks permission for the locks it requires. The second part is where the transaction acquires all the locks. As soon as the transaction releases its first lock, the third phase starts. In this phase, the transaction cannot demand any new locks; it only releases the acquired locks.

    Two-phase locking has two phases, one is growing, where all the locks are being acquired by the transaction;
    and the second phase is shrinking, where the locks held by the transaction are being released.

    To claim an exclusive (write) lock, a transaction must first acquire a shared (read) lock and then upgrade it to an exclusive lock.

4) Strict Two-Phase Locking

    The first phase of Strict-2PL is same as 2PL. After acquiring all the locks in the first phase, the transaction continues to execute normally. But in contrast to 2PL, Strict-2PL does not release a lock after using it. Strict-2PL holds all the locks until the commit point and releases all the locks at a time.


Timestamp-based Protocols
The most commonly used concurrency protocol is the timestamp based protocol. This protocol uses either system time or logical counter as a timestamp.

Lock-based protocols manage the order between the conflicting pairs among transactions at the time of execution,
whereas timestamp-based protocols start working as soon as a transaction is created.


Timestamp Ordering Protocol
The timestamp-ordering protocol ensures serializability among transactions in their conflicting read and write operations. This is the responsibility of the protocol system that the conflicting pair of tasks should be executed according to the timestamp values of the transactions.
    The timestamp of transaction Ti is denoted as TS(Ti).
    Read time-stamp of data-item X is denoted by R-timestamp(X).
    Write time-stamp of data-item X is denoted by W-timestamp(X).


Timestamp ordering protocol works as follows −

If a transaction Ti issues a read(X) operation −
    If TS(Ti) < W-timestamp(X)
        Operation rejected.
    If TS(Ti) >= W-timestamp(X)
        Operation executed.
    All data-item timestamps updated.

If a transaction Ti issues a write(X) operation −
    If TS(Ti) < R-timestamp(X)
        Operation rejected.
    If TS(Ti) < W-timestamp(X)
        Operation rejected and Ti rolled back.
    Otherwise, operation executed.


## DBMS - Deadlock



# Backup and Recovery

## DBMS - Data Backup

Loss of Volatile Storage
A volatile storage like RAM stores all the active logs, disk buffers, and related data.
In addition, it stores all the transactions that are being currently executed. What happens if such a volatile storage crashes abruptly? It would obviously take away all the logs and active copies of the database. It makes recovery almost impossible, as everything that is required to recover the data is lost.


Following techniques may be adopted in case of loss of volatile storage −
    - We can have checkpoints at multiple stages so as to save the contents of the database periodically.

    - A state of active database in the volatile memory can be periodically dumped onto a stable storage, which may also contain logs and active transactions and buffer blocks.

    - <dump> can be marked on a log file, whenever the database contents are dumped from a non-volatile memory to a stable one.

Recovery
    - When the system recovers from a failure, it can restore the latest dump.

    - It can maintain a redo-list and an undo-list as checkpoints.

    - It can recover the system by consulting undo-redo lists to restore the state of all transactions up to the last checkpoint.


Remote backup &minu; Here a backup copy of the database is stored at a remote location from where it can be restored in case of a catastrophe.

Alternatively, database backups can be taken on magnetic tapes and stored at a safer place. This backup can later be transferred onto a freshly installed database to bring it to the point of backup.

Remote Backup
Remote backup provides a sense of security in case the primary location where the database is located gets destroyed. Remote backup can be offline or real-time or online. In case it is offline, it is maintained manually.



## DBMS - Data Recovery

Crash Recovery
DBMS is a highly complex system with hundreds of transactions being executed every second. The durability and robustness of a DBMS depends on its complex architecture and its underlying hardware and system software. If it fails or crashes amid transactions, it is expected that the system would follow some sort of algorithm or techniques to recover lost data.


Failure Classification
To see where the problem has occurred, we generalize a failure into various categories, as follows −

1) Transaction failure
A transaction has to abort when it fails to execute or when it reaches a point from where it can’t go any further. This is called transaction failure where only a few transactions or processes are hurt.


2） System Crash
There are problems − external to the system − that may cause the system to stop abruptly and cause the system to crash. For example, interruptions in power supply may cause the failure of underlying hardware or software failure.


3）Disk Failure
In early days of technology evolution, it was a common problem where hard-disk drives or storage drives used to fail frequently.

Disk failures include formation of bad sectors, unreachability to the disk, disk head crash or any other failure, which destroys all or a part of disk storage.



Storage Structure
We have already described the storage system. In brief, the storage structure can be divided into two categories −

Volatile storage - a volatile storage cannot survive system crashes.
Non-volatile storage − These memories are made to survive system crashes. They are huge in data storage capacity, but slower in accessibility. Examples may include hard-disks, magnetic tapes, flash memory, and non-volatile (battery backed up) RAM.



Recovery and Atomicity

When a DBMS recovers from a crash, it should maintain the following −

It should check the states of all the transactions, which were being executed.

A transaction may be in the middle of some operation; the DBMS must ensure the atomicity of the transaction in this case.

It should check whether the transaction can be completed now or it needs to be rolled back.

No transactions would be allowed to leave the DBMS in an inconsistent state.



There are two types of techniques, which can help a DBMS in recovering as well as maintaining the atomicity of a transaction −

    Maintaining the logs of each transaction, and writing them onto some stable storage before actually modifying the database.

    Maintaining shadow paging（copy-on-write）, where the changes are done on a volatile memory, and later, the actual database is updated.


Log-based Recovery

Log is a sequence of records, which maintains the records of actions performed by a transaction. It is important that the logs are written prior to the actual modification and stored on a stable storage media, which is failsafe.

Log-based recovery works as follows −

    - The log file is kept on a stable storage media.

    - When a transaction enters the system and starts execution, it writes a log about it.
    <Tn, Start>

    - When the transaction modifies an item X, it write logs as follows −
    <Tn, X, V1, V2>
    It reads Tn has changed the value of X, from V1 to V2.

    - When the transaction finishes, it logs −
    <Tn, commit>


The database can be modified using two approaches −
    Deferred database modification − All logs are written on to the stable storage and the database is updated when a transaction commits.

    Immediate database modification − Each log follows an actual database modification. That is, the database is modified immediately after every operation.




Recovery with Concurrent Transactions
When more than one transaction are being executed in parallel, the logs are interleaved. At the time of recovery, it would become hard for the recovery system to backtrack all logs, and then start recovering. To ease this situation, most modern DBMS use the concept of 'checkpoints'.



Checkpoint
Keeping and maintaining logs in real time and in real environment may fill out all the memory space available in the system. As time passes, the log file may grow too big to be handled at all. Checkpoint is a mechanism where all the previous logs are removed from the system and stored permanently in a storage disk. Checkpoint declares a point before which the DBMS was in consistent state, and all the transactions were committed.

Recovery
When a system with concurrent transactions crashes and recovers, it behaves in the following manner −
    The recovery system reads the logs backwards from the end to the last checkpoint.

    It maintains two lists, an undo-list and a redo-list.

    If the recovery system sees a log with <Tn, Start> and <Tn, Commit> or just <Tn, Commit>, it puts the transaction in the redo-list.

    If the recovery system sees a log with <Tn, Start> but no commit or abort log found, it puts the transaction in undo-list.



All the transactions in the undo-list are then undone and their logs are removed. All the transactions in the redo-list and their previous logs are removed and then redone before saving their logs.

































